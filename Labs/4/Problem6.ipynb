{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Load Cifar10 as datasets\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "training_data = datasets.CIFAR10(\n",
        "    root = 'data',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.CIFAR10(\n",
        "    root = \"data\",\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = ToTensor()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O030M5MQ3GU",
        "outputId": "9c399f82-8a9b-4f57-afae-4cfa0a568a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders from them\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size = 100, shuffle = True)\n",
        "test_dataloader = DataLoader(test_data, batch_size = 100, shuffle = True)"
      ],
      "metadata": {
        "id": "E0-UMYFIQ7bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a basic neural net to compute results\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN_1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 3)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "    self.l1 = nn.Linear(16 * 5 * 5, 120)\n",
        "    self.l2 = nn.Linear(120, 84)\n",
        "    self.l3 = nn.Linear(84, 10)\n",
        "    # fully connected layer, output 10 classes\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x))) # perform the convolution, then the activation function on the layer\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = torch.flatten(x,1)\n",
        "    x = F.relu(self.l1(x))\n",
        "    x = F.relu(self.l2(x))\n",
        "    x = self.l3(x)\n",
        "    # Flatten the out\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "rtGEsGg6RNz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2)\n",
        "    )\n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2)\n",
        "    )\n",
        "    self.out = nn.Linear(32 * 8 * 8, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x)) # perform the convolution\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x= x.view(x.size(0), -1)\n",
        "    output = self.out(x)\n",
        "    # Flatten the out\n",
        "    return output"
      ],
      "metadata": {
        "id": "jH-SHH8eJW_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_3(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2)\n",
        "    )\n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2)\n",
        "    )\n",
        "    self.conv3 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 1, padding = 1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2)\n",
        "        )\n",
        "    self.out = nn.Linear(64 * 4 * 4, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x)) # perform the convolution\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x= x.view(x.size(0), -1)\n",
        "    output = self.out(x)\n",
        "    # Flatten the out\n",
        "    return output"
      ],
      "metadata": {
        "id": "T8mXEOV4XKaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RuntimeError                              Traceback (most recent call last)\n",
        "<ipython-input-14-1aec4c419434> in <module>\n",
        "     14   train(dataloader = train_dataloader, \n",
        "     15         model = model1, loss_func = loss_func,\n",
        "---> 16         optimizer = optimizer)\n",
        "\n",
        "7 frames\n",
        "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py in _conv_forward(self, input, weight, bias)\n",
        "    452                             _pair(0), self.dilation, self.groups)\n",
        "    453         return F.conv2d(input, weight, bias, self.stride,\n",
        "--> 454                         self.padding, self.dilation, self.groups)\n",
        "    455 \n",
        "    456     def forward(self, input: Tensor) -> Tensor:\n",
        "\n",
        "RuntimeError: Given groups=1, weight of size [16, 1, 5, 5], expected input[100, 3, 32, 32] to have 1 channels, but got 3 channels instead"
      ],
      "metadata": {
        "id": "KJyqT1tKxNqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_func, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "\n",
        "  # Train the model on every batch; perform forward then backward\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    #X, y = X.to(device), y.to(device) # ??\n",
        "\n",
        "    # Compute the prediction error\n",
        "    pred = model(X)\n",
        "    loss = loss_func(pred, y)\n",
        " \n",
        "    # Clear the previous gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # computes graidents\n",
        "    loss.backward()\n",
        "\n",
        "    # Applies gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    # Every 100 batches, print the loss\n",
        "    if batch % 100 == 0:\n",
        "        loss, current_batch = loss.item(), batch * len(X)\n",
        "        # print(\"For the batch \" + str(current_batch) + \" the loss is \" + str(loss))"
      ],
      "metadata": {
        "id": "VC1_auEDSDzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the accuracy of the neural net\n",
        "import torch\n",
        "def test(dataloader, model, loss_func):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  model.eval() \n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      pred = model(X)\n",
        "      test_loss += loss_func(pred, y).item() # Add the loss amount\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "                   \n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(\"The test error: \")\n",
        "  print(\"\\t Accuracy: \", 100 * correct)\n",
        "  print(\"\\t Avg Loss: \", test_loss)"
      ],
      "metadata": {
        "id": "S-GLA3HhSOwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create models\n",
        "model1 = CNN_1()\n",
        "\n",
        "# create an optimizer, ADAM first\n",
        "optimizer = torch.optim.Adam(model1.parameters(), lr = .001)\n",
        "\n",
        "# create a loss function\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the nn\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train(train_dataloader, model1, loss_func, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_ZwtfipSR5a",
        "outputId": "b88b2efc-cf5d-44c6-a509-b7b17a17f3e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the batch 0 the loss is 2.299276351928711\n",
            "For the batch 10000 the loss is 1.936888575553894\n",
            "For the batch 20000 the loss is 1.7993218898773193\n",
            "For the batch 30000 the loss is 1.822350025177002\n",
            "For the batch 40000 the loss is 1.5451610088348389\n",
            "For the batch 0 the loss is 1.571778416633606\n",
            "For the batch 10000 the loss is 1.5351146459579468\n",
            "For the batch 20000 the loss is 1.4890764951705933\n",
            "For the batch 30000 the loss is 1.5546023845672607\n",
            "For the batch 40000 the loss is 1.3030287027359009\n",
            "For the batch 0 the loss is 1.592533826828003\n",
            "For the batch 10000 the loss is 1.344964861869812\n",
            "For the batch 20000 the loss is 1.5158953666687012\n",
            "For the batch 30000 the loss is 1.5160419940948486\n",
            "For the batch 40000 the loss is 1.4254355430603027\n",
            "For the batch 0 the loss is 1.4004729986190796\n",
            "For the batch 10000 the loss is 1.5541415214538574\n",
            "For the batch 20000 the loss is 1.6625869274139404\n",
            "For the batch 30000 the loss is 1.3081164360046387\n",
            "For the batch 40000 the loss is 1.4563826322555542\n",
            "For the batch 0 the loss is 1.2208447456359863\n",
            "For the batch 10000 the loss is 1.4693814516067505\n",
            "For the batch 20000 the loss is 1.3362923860549927\n",
            "For the batch 30000 the loss is 1.4220340251922607\n",
            "For the batch 40000 the loss is 1.3664873838424683\n",
            "For the batch 0 the loss is 1.316622018814087\n",
            "For the batch 10000 the loss is 1.2515549659729004\n",
            "For the batch 20000 the loss is 1.1314622163772583\n",
            "For the batch 30000 the loss is 1.0440568923950195\n",
            "For the batch 40000 the loss is 1.216183066368103\n",
            "For the batch 0 the loss is 1.1321431398391724\n",
            "For the batch 10000 the loss is 1.2824065685272217\n",
            "For the batch 20000 the loss is 1.1223249435424805\n",
            "For the batch 30000 the loss is 1.3030449151992798\n",
            "For the batch 40000 the loss is 1.284102201461792\n",
            "For the batch 0 the loss is 1.1276030540466309\n",
            "For the batch 10000 the loss is 1.1828796863555908\n",
            "For the batch 20000 the loss is 1.2308661937713623\n",
            "For the batch 30000 the loss is 1.160098671913147\n",
            "For the batch 40000 the loss is 1.3505580425262451\n",
            "For the batch 0 the loss is 1.379661202430725\n",
            "For the batch 10000 the loss is 1.2248315811157227\n",
            "For the batch 20000 the loss is 1.243186354637146\n",
            "For the batch 30000 the loss is 1.3080625534057617\n",
            "For the batch 40000 the loss is 1.3102400302886963\n",
            "For the batch 0 the loss is 1.071969747543335\n",
            "For the batch 10000 the loss is 1.0923106670379639\n",
            "For the batch 20000 the loss is 1.4114789962768555\n",
            "For the batch 30000 the loss is 1.0710145235061646\n",
            "For the batch 40000 the loss is 1.2006863355636597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test results for model 1, using adam optimizer\n",
        "test(test_dataloader, model1, loss_func)"
      ],
      "metadata": {
        "id": "_DQM3_vXSaMi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efa7cc51-9a2a-42ec-ffa2-1e621eb0dc9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The test error: \n",
            "\t Accuracy:  55.730000000000004\n",
            "\t Avg Loss:  1.2278882354497909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train and test model 2\n",
        "model2 = CNN_2() #cnn 2 is a deeper extension of cnn1 with an additional convolutional layer\n",
        "optimizer2 = torch.optim.Adam(model2.parameters(), lr = .001)\n",
        "for epoch in range(epochs):\n",
        "  train(train_dataloader, model2, loss_func, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "O6Aw7DknKHeQ",
        "outputId": "3901782b-f01b-44bf-c375-88f25e852a2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-2b10f7dbd195>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-79-1253a1868b41>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_func, optimizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Compute the prediction error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-1f301dcd4a58>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# perform the convolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Flatten the out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (25600x8 and 2048x10)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(test_dataloader, model2, loss_func)"
      ],
      "metadata": {
        "id": "ALW2ruNEKMhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = [.0001, .001, .01]\n",
        "momentums = [.7,.8,.9]\n",
        "\n",
        "outputs = []\n",
        "for l in lr:\n",
        "  # create an optimizer, ADAM first\n",
        "  optimizer = torch.optim.Adam(model1.parameters(), lr = l)\n",
        "  for epoch in range(epochs):\n",
        "    train(train_dataloader, model1, loss_func, optimizer)\n",
        "  test(test_dataloader, model1, loss_func)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZObNPsIKd9V",
        "outputId": "453a3247-7116-4765-e2e0-f664f9aaaa13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The test error: \n",
            "\t Accuracy:  59.550000000000004\n",
            "\t Avg Loss:  1.1599075984954834\n",
            "The test error: \n",
            "\t Accuracy:  59.61\n",
            "\t Avg Loss:  1.1560079091787339\n",
            "The test error: \n",
            "\t Accuracy:  54.779999999999994\n",
            "\t Avg Loss:  1.2966955476999282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly changing the learning rate greatly swings the accuacy of the CNN. Too high of a learning rate + too low of a learning rate gives us a more poor accuracy."
      ],
      "metadata": {
        "id": "24h_9dnWV3rq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EERH8w2tV0hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in momentums:\n",
        "  # create an optimizer, ADAM first\n",
        "  optimizer = torch.optim.SGD(model1.parameters(), lr=0.01, momentum=m)\n",
        "  # optimizer = torch.optim.Adam(model1.parameters(), lr = .001, momentum = m)\n",
        "  for epoch in range(epochs):\n",
        "    train(train_dataloader, model1, loss_func, optimizer)\n",
        "  test(test_dataloader, model1, loss_func)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMpGVk-URXoB",
        "outputId": "b9301554-71d0-495e-9788-ad5f1ae1b230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The test error: \n",
            "\t Accuracy:  57.85\n",
            "\t Avg Loss:  1.232039583325386\n",
            "The test error: \n",
            "\t Accuracy:  57.97\n",
            "\t Avg Loss:  1.243187518119812\n",
            "The test error: \n",
            "\t Accuracy:  56.54\n",
            "\t Avg Loss:  1.3101933073997498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, momentum does effect our accuracy greatly. Too much/little momentum will give us poor accuracy."
      ],
      "metadata": {
        "id": "ccMSrNC3Xtgh"
      }
    }
  ]
}