{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"6wuXcLuNzfPk","executionInfo":{"status":"ok","timestamp":1665453797286,"user_tz":300,"elapsed":1235,"user":{"displayName":"Laith Altarabishi","userId":"02910282525670990534"}}},"outputs":[],"source":["from pathlib import Path\n","import requests\n","\n","DATA_PATH = Path(\"data\")\n","PATH = DATA_PATH / \"mnist\"\n","\n","PATH.mkdir(parents=True, exist_ok=True)\n","\n","URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n","FILENAME = \"mnist.pkl.gz\"\n","\n","if not (PATH / FILENAME).exists():\n","        content = requests.get(URL + FILENAME).content\n","        (PATH / FILENAME).open(\"wb\").write(content)"]},{"cell_type":"markdown","source":["Importing hte MNIST Dataset"],"metadata":{"id":"bTqBsmpJzkjp"}},{"cell_type":"code","source":["import pickle\n","import gzip\n","\n","with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n","        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"],"metadata":{"id":"CXriajXmznr2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from matplotlib import pyplot\n","import numpy as np\n","\n","pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n","print(x_train.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"w9slg14hztB1","outputId":"d0c2c2b4-0948-4539-eb02-52ff9f486b77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(50000, 784)\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["Here is an example image from the dataset.\n","The link we are using:\n","\n","https://pytorch.org/tutorials/beginner/nn_tutorial.html\n"],"metadata":{"id":"C0XtK0YGzwZt"}},{"cell_type":"code","source":["# Pytorch uses tensors instead of arrays, need to convert\n","import torch\n","\n","x_train, y_train, x_valid, y_valid = map(\n","    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",")\n","n, c = x_train.shape\n","print(x_train, y_train)\n","print(x_train.shape)\n","print(y_train.min(), y_train.max())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BuLE8EcVzy5F","outputId":"c5604384-cbce-4b5a-c0fc-7ee1d04b62aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n","torch.Size([50000, 784])\n","tensor(0) tensor(9)\n"]}]},{"cell_type":"code","source":["# lets first create a neural net from scratch\n","import math\n","# Initialize the weights, then set requires gradient( for back propogation)\n","weights = torch.randn(784, 10) / math.sqrt(784)\n","weights.requires_grad_() # Inplace operation\n","bias  = torch.zeros(10, requires_grad = True)\n","\n","weights, bias"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QwOlJnBq0G7c","outputId":"43890e98-cf44-4cf9-db0c-2f862f6b1a94"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[-0.0322,  0.0099,  0.0794,  ..., -0.0099,  0.0152, -0.0367],\n","         [-0.0054, -0.0047,  0.0598,  ..., -0.0053, -0.0036, -0.0309],\n","         [-0.0465,  0.0022,  0.0163,  ..., -0.0477, -0.0429,  0.0450],\n","         ...,\n","         [ 0.0436, -0.0007, -0.0070,  ..., -0.0268,  0.0167, -0.0257],\n","         [ 0.0090, -0.0108, -0.0142,  ...,  0.0289, -0.0250, -0.0330],\n","         [ 0.0624,  0.0381,  0.0823,  ..., -0.0139, -0.0078, -0.0100]],\n","        requires_grad=True),\n"," tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True))"]},"metadata":{},"execution_count":102}]},{"cell_type":"code","source":["# Create a simple model and loss function\n","def log_softmax(x):\n","    return x - x.exp().sum(-1).log().unsqueeze(-1)\n","\n","def model(xb):\n","    return log_softmax(xb @ weights + bias) # @ indicates matrix mulitplication"],"metadata":{"id":"7B4wFFbs02__"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" # Create a batch\n","\n"," bs  = 64\n"," xb = x_train[0:bs] # Pick 64 images\n"," preds = model(xb) # Create predictions\n"," preds[0], preds.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6QrnUQv01HWp","outputId":"e8173fa4-00f1-45d8-a70b-d3d3b4aac45b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([-2.6597, -2.9534, -2.1368, -2.2883, -1.9389, -2.0847, -2.2356, -2.2323,\n","         -2.2617, -2.6337], grad_fn=<SelectBackward0>), torch.Size([64, 10]))"]},"metadata":{},"execution_count":104}]},{"cell_type":"code","source":["# Use the negative log liklihood to compute loss (cross entropy)\n","def nll(input, target):\n","    return -input[range(target.shape[0]), target].mean()\n","\n","loss_func = nll"],"metadata":{"id":"CwP1I7MA2LGU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["yb = y_train[0:bs] # Get the actual classifications\n","loss_func(preds, yb) # Computes the loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ruJhjdQV2kgb","outputId":"2ad1917c-e498-4c82-a815-f7a0750137b9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2.3168, grad_fn=<NegBackward0>)"]},"metadata":{},"execution_count":106}]},{"cell_type":"code","source":["# Function to compute the accuracy of the model\n","def accuracy(out, yb):\n","    preds = torch.argmax(out, dim=1)\n","    return (preds == yb).float().mean()"],"metadata":{"id":"ANFIITmn2teg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accuracy(preds, yb)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gUdzkSfN2zy4","outputId":"c7b267de-2062-4b9b-e070-0179edcbf97c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.1406)"]},"metadata":{},"execution_count":108}]},{"cell_type":"code","source":["# We can now run a trainining loop\n","# 1 - select a mini batch of data\n","# 2 - use the model to make predictions\n","# 3 - calculate the loss\n","# 4 - loss.backward() updatesthe gradients of the model, aka the weights and the bias\n","\n","from IPython.core.debugger import set_trace\n","\n","lr = 0.5  # learning rate\n","epochs = 2  # how many epochs to train for\n","\n","for epoch in range(epochs):\n","    for i in range((n - 1) // bs + 1):\n","        #         set_trace()\n","        start_i = i * bs\n","        end_i = start_i + bs\n","        xb = x_train[start_i:end_i] # current input batch\n","        yb = y_train[start_i:end_i] # current output batch\n","        pred = model(xb) # Current predictions\n","        loss = loss_func(pred, yb) # current loss\n","\n","        loss.backward() # update the weights and the biases\n","        with torch.no_grad():\n","            weights -= weights.grad * lr\n","            bias -= bias.grad * lr\n","            weights.grad.zero_()\n","            bias.grad.zero_()"],"metadata":{"id":"FLWmFdi525pr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lets see how accurate the model is now\n","accuracy(model(xb), yb)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8mtNPUr83yCK","outputId":"0454405d-a818-40a9-9c75-0ccd81700b49"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.)"]},"metadata":{},"execution_count":110}]},{"cell_type":"code","source":["# Lets refactor the code, but use the torch.nn framework\n","\n","# This will make the code: shorter, easier to understand, and more flexible\n","\n","import torch.nn.functional as F\n","\n","loss_func = F.cross_entropy # the loss function\n","\n","def model(xb):\n","  return xb @ weights + bias # matrix multiplcation"],"metadata":{"id":"33hKoCu034Nt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_func(model(xb), yb), accuracy(model(xb), yb)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lV3CcDLP4pxi","outputId":"86e0f639-8d2b-43ab-86f9-e23f1e8f500c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.0785, grad_fn=<NllLossBackward0>), tensor(1.))"]},"metadata":{},"execution_count":112}]},{"cell_type":"code","source":["from torch import nn\n","\n","class Mnist_Logistic(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.weights  = nn.Parameter(torch.randn(784, 10) / math.sqrt(784)) # assign the initial weights of the model\n","    self.bias = nn.Parameter(torch.zeros(10))\n","\n","  def forward(self, xb):\n","    return xb @ self.weights + self.bias # Compute the simple nn forward\n","model = Mnist_Logistic()\n"],"metadata":{"id":"ExSLz7ur45G4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_func(model(xb), yb)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5FiRslRZ5oSD","outputId":"180614b5-77e6-47ff-800e-c332aa31305b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2.4028, grad_fn=<NllLossBackward0>)"]},"metadata":{},"execution_count":114}]},{"cell_type":"code","source":["# Create a fit function\n","def fit():\n","    for epoch in range(epochs):\n","        for i in range((n - 1) // bs + 1):\n","            start_i = i * bs\n","            end_i = start_i + bs\n","            xb = x_train[start_i:end_i]\n","            yb = y_train[start_i:end_i]\n","            pred = model(xb)\n","            loss = loss_func(pred, yb)\n","\n","            loss.backward()\n","            with torch.no_grad():\n","                for p in model.parameters():\n","                    p -= p.grad * lr\n","                model.zero_grad()\n","\n","fit()"],"metadata":{"id":"6-HJmMQg5y9z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_func(model(xb), yb)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tAHgMWDF5858","outputId":"33f3ebdc-f794-4e94-9d57-e4fb0d46b3e1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.0834, grad_fn=<NllLossBackward0>)"]},"metadata":{},"execution_count":116}]},{"cell_type":"code","source":["# Lets refactor agan, using nn.Linear\n","\n","class Mnist_Logistic(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.lin = nn.Linear(784, 10) # Creates linear layer\n","  def forward(self, xb):\n","    return self.lin(xb) # presto, matrix calculation again"],"metadata":{"id":"_mbJjUcC6A1S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Mnist_Logistic()\n","loss_func(model(xb), yb)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l-4XTdO96YT0","outputId":"851bb2b8-8a50-4568-cd3b-5751d229aec1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2.2637, grad_fn=<NllLossBackward0>)"]},"metadata":{},"execution_count":118}]},{"cell_type":"code","source":["fit()\n","loss_func(model(xb), yb)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w5EmmU416etA","outputId":"6b9ee4ef-f55a-4158-ab04-8a530f39cf89"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.0811, grad_fn=<NllLossBackward0>)"]},"metadata":{},"execution_count":119}]},{"cell_type":"code","source":["# Refactor the propogration process again\n","from torch import optim\n","def get_model():\n","    model = Mnist_Logistic()\n","    return model, optim.SGD(model.parameters(), lr=lr) # Create an optimizer, passing in model and learning rate\n","\n","model, opt = get_model()\n","print(loss_func(model(xb), yb))\n","\n","for epoch in range(epochs):\n","    for i in range((n - 1) // bs + 1):\n","        start_i = i * bs\n","        end_i = start_i + bs\n","        xb = x_train[start_i:end_i]\n","        yb = y_train[start_i:end_i]\n","        pred = model(xb)\n","        loss = loss_func(pred, yb)\n","\n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()\n","\n","print(loss_func(model(xb), yb))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DoXd5ajQ6moa","outputId":"3d66ba11-5601-4291-ab0b-6a2c0b5fe00e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(2.3220, grad_fn=<NllLossBackward0>)\n","tensor(0.0806, grad_fn=<NllLossBackward0>)\n"]}]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset"],"metadata":{"id":"nrIopbA87EzK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Can combine X_train, y_train into a single dataset\n","\n","train_ds = TensorDataset(x_train, y_train)\n","model, opt = get_model()\n","\n","for epoch in range(epochs):\n","    for i in range((n - 1) // bs + 1):\n","        xb, yb = train_ds[i * bs: i * bs + bs]\n","        pred = model(xb)\n","        loss = loss_func(pred, yb)\n","\n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()\n","\n","print(loss_func(model(xb), yb))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ddkLQCLj6uJJ","outputId":"202c228f-13d6-4aca-c680-83f8044243cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.0814, grad_fn=<NllLossBackward0>)\n"]}]},{"cell_type":"markdown","source":[" /"],"metadata":{"id":"Z24qlsUkJxLu"}},{"cell_type":"markdown","source":["/"],"metadata":{"id":"-GRf0wD-JxNx"}},{"cell_type":"markdown","source":["/"],"metadata":{"id":"hDPqmlSYJxQb"}},{"cell_type":"markdown","source":["/"],"metadata":{"id":"kZup9c0JJxS2"}},{"cell_type":"markdown","source":["/"],"metadata":{"id":"UjeYxssCJxVk"}},{"cell_type":"markdown","source":["//////////// Done with Experimentation: Here is the work for problem 5 /////////////"],"metadata":{"id":"0Hj6N5ckJoNT"}},{"cell_type":"code","source":["# Dataset and Dataloader make the algorithm much simpler\n","from torch.utils.data import Dataset\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import matplotlib.pyplot as plt\n","\n","training_data = datasets.MNIST(\n","    root = 'data',\n","    train = True,\n","    download = True,\n","    transform = ToTensor()\n",")\n","\n","test_data = datasets.MNIST(\n","    root = \"data\",\n","    train = False,\n","    download = True,\n","    transform = ToTensor()\n",")"],"metadata":{"id":"Ugknbcpe9TGS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data loader helps partition the dataset into batches\n","\n","from torch.utils.data import DataLoader\n","\n","train_dataloader = DataLoader(training_data, batch_size = 100, shuffle = True)\n","trest_dataloader = DataLoader(test_data, batch_size = 100, shuffle = True)"],"metadata":{"id":"t5MgFMz6lvp5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Two layer neural net using nn.Module\n","import torch.nn as nn\n","import torch.nn.functional as F\n","class Model(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.conv1 = nn.Sequential(         \n","            nn.Conv2d(\n","                in_channels=1,              \n","                out_channels=16,            \n","                kernel_size=5,              \n","                stride=1,                   \n","                padding=2,                  \n","            ),                              \n","            nn.ReLU(),                      \n","            nn.MaxPool2d(kernel_size=2),    \n","        )\n","    self.conv2 = nn.Sequential(         \n","            nn.Conv2d(16, 32, 5, 1, 2),     \n","            nn.ReLU(),                      \n","            nn.MaxPool2d(2),                \n","        )\n","    # fully connected layer, output 10 classes\n","    self.out = nn.Linear(32 * 7 * 7, 10)\n","  def forward(self, x):\n","    x = F.relu(self.conv1(x)) # perform the convolution, then the activation function on the layer\n","    x = F.relu(self.conv2(x))\n","\n","    # Flatten the out\n","    x= x.view(x.size(0), -1)\n","    output = self.out(x)\n","    return output\n"],"metadata":{"id":"3tcluNfYmfU-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Model()\n","loss_func = nn.CrossEntropyLoss() # Loss function\n","optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)"],"metadata":{"id":"lWkRR-a5oAAP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(dataloader, model, loss_func, optimizer):\n","  size = len(dataloader.dataset)\n","  model.train()\n","\n","  # Train the model on every batch; perform forward then backward\n","  for batch, (X, y) in enumerate(dataloader):\n","    #X, y = X.to(device), y.to(device) # ??\n","\n","    # Compute the prediction error\n","    pred = model(X)\n","    loss = loss_func(pred, y)\n"," \n","    # Clear the previous gradients\n","    optimizer.zero_grad()\n","\n","    # computes graidents\n","    loss.backward()\n","\n","    # Applies gradients\n","    optimizer.step()\n","\n","    # Every 100 batches, print the loss\n","    if batch % 100 == 0:\n","        loss, current_batch = loss.item(), batch * len(X)\n","        print(\"For the batch \" + str(current_batch) + \" the loss is \" + str(loss))"],"metadata":{"id":"hOl43uqaok6Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(train_dataloader, model, loss_func, optimizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g0uTyjQW0FY1","outputId":"87e95ecd-fc08-47e1-ce84-8734e2d06fd5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["For the batch 0 the loss is 2.310359001159668\n","For the batch 10000 the loss is 2.2853307723999023\n","For the batch 20000 the loss is 2.2655158042907715\n","For the batch 30000 the loss is 2.243856430053711\n","For the batch 40000 the loss is 2.2306816577911377\n","For the batch 50000 the loss is 2.195636749267578\n"]}]},{"cell_type":"code","source":["# Lets try again with a differeent optimizer\n","optimizer2 = optim.Adam(model.parameters(), lr = 0.01)\n","train(train_dataloader, model, loss_func, optimizer)\n","for i in range(5):\n","  train(train_dataloader, model, loss_func, optimizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zQdovygs8cXp","outputId":"a8573919-b8a2-4bc2-e7b8-5fc34171aa11"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["For the batch 0 the loss is 0.438822478055954\n","For the batch 10000 the loss is 0.2501527965068817\n","For the batch 20000 the loss is 0.3698313236236572\n","For the batch 30000 the loss is 0.31150364875793457\n","For the batch 40000 the loss is 0.22245045006275177\n","For the batch 50000 the loss is 0.25717222690582275\n","For the batch 0 the loss is 0.3834066092967987\n","For the batch 10000 the loss is 0.36567923426628113\n","For the batch 20000 the loss is 0.40264004468917847\n","For the batch 30000 the loss is 0.219085693359375\n","For the batch 40000 the loss is 0.36899739503860474\n","For the batch 50000 the loss is 0.2682541608810425\n","For the batch 0 the loss is 0.26861655712127686\n","For the batch 10000 the loss is 0.24706143140792847\n","For the batch 20000 the loss is 0.2613620460033417\n","For the batch 30000 the loss is 0.3429142236709595\n","For the batch 40000 the loss is 0.2476826161146164\n","For the batch 50000 the loss is 0.3573053479194641\n","For the batch 0 the loss is 0.17889125645160675\n","For the batch 10000 the loss is 0.29840701818466187\n","For the batch 20000 the loss is 0.23351144790649414\n","For the batch 30000 the loss is 0.34686988592147827\n","For the batch 40000 the loss is 0.37900927662849426\n","For the batch 50000 the loss is 0.3086456060409546\n","For the batch 0 the loss is 0.238690584897995\n","For the batch 10000 the loss is 0.20520830154418945\n","For the batch 20000 the loss is 0.2979625463485718\n","For the batch 30000 the loss is 0.4675035774707794\n","For the batch 40000 the loss is 0.3205573558807373\n","For the batch 50000 the loss is 0.4137105643749237\n","For the batch 0 the loss is 0.23426170647144318\n","For the batch 10000 the loss is 0.2502574920654297\n","For the batch 20000 the loss is 0.23384332656860352\n","For the batch 30000 the loss is 0.2159603238105774\n","For the batch 40000 the loss is 0.2795983552932739\n","For the batch 50000 the loss is 0.17425012588500977\n"]}]},{"cell_type":"code","source":["# Determine the accuracy of the neural net\n","def test(dataloader, model, loss_func):\n","  size = len(dataloader.dataset)\n","  num_batches = len(dataloader)\n","  model.eval() \n","  test_loss, correct = 0, 0\n","  with torch.no_grad():\n","    for X, y in dataloader:\n","      pred = model(X)\n","      test_loss += loss_func(pred, y).item() # Add the loss amount\n","      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","                   \n","  test_loss /= num_batches\n","  correct /= size\n","  print(\"The test error: \")\n","  print(\"\\t Accuracy: \", 100 * correct)\n","  print(\"\\t Avg Loss: \", test_loss)"],"metadata":{"id":"08vx1Ji_EoDl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test(train_dataloader, model, loss_func)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fu9_pLV-EgG2","outputId":"75c2e1bb-077a-4b2c-9309-5c60533eff34"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The test error: \n","\t Accuracy:  92.2\n","\t Avg Loss:  0.2689057399208347\n"]}]},{"cell_type":"markdown","source":["Here is the performance running the CNN on MNIST many times, using a SGD optimizer"],"metadata":{"id":"h1DLqVsZIE5I"}},{"cell_type":"code","source":["# Try repeating with the adam\n","# Lets try again with a differeent optimizer\n","\n","model = Model()\n","optimizer2 = optim.Adam(model.parameters(), lr = 0.001)\n","for i in range(5):\n","  train(train_dataloader, model, loss_func, optimizer2)\n","test(train_dataloader, model, loss_func)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ewps-1Z0HT3B","outputId":"bd113090-eb1f-49d4-ad82-27b82ea17538"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["For the batch 0 the loss is 2.3051483631134033\n","For the batch 10000 the loss is 0.18956734240055084\n","For the batch 20000 the loss is 0.13590861856937408\n","For the batch 30000 the loss is 0.1105237677693367\n","For the batch 40000 the loss is 0.10080530494451523\n","For the batch 50000 the loss is 0.030715016648173332\n","For the batch 0 the loss is 0.023775439709424973\n","For the batch 10000 the loss is 0.04209733381867409\n","For the batch 20000 the loss is 0.045487333089113235\n","For the batch 30000 the loss is 0.10693678259849548\n","For the batch 40000 the loss is 0.020700059831142426\n","For the batch 50000 the loss is 0.08158788830041885\n","For the batch 0 the loss is 0.10063817352056503\n","For the batch 10000 the loss is 0.03825485333800316\n","For the batch 20000 the loss is 0.038078926503658295\n","For the batch 30000 the loss is 0.07640007138252258\n","For the batch 40000 the loss is 0.02452344447374344\n","For the batch 50000 the loss is 0.01141931489109993\n","For the batch 0 the loss is 0.03381400927901268\n","For the batch 10000 the loss is 0.0456770621240139\n","For the batch 20000 the loss is 0.022311225533485413\n","For the batch 30000 the loss is 0.04315028712153435\n","For the batch 40000 the loss is 0.08908654004335403\n","For the batch 50000 the loss is 0.006316243205219507\n","For the batch 0 the loss is 0.005510319024324417\n","For the batch 10000 the loss is 0.048499416559934616\n","For the batch 20000 the loss is 0.06391791254281998\n","For the batch 30000 the loss is 0.008034653030335903\n","For the batch 40000 the loss is 0.05145660415291786\n","For the batch 50000 the loss is 0.033997487276792526\n","The test error: \n","\t Accuracy:  99.41666666666666\n","\t Avg Loss:  0.021174292914414156\n"]}]}]}